{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n3lnWjvI83ix"
   },
   "source": [
    "# Filtado de mensajes spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción del problema real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La recepción de publicidad no deseada a traves mensajes de texto usando SMS (Short Message Service) es un problema que afecta a muchos usuarios de teléfonos móviles. El problema radica en que los usuarios deben pagar por los mesajes recibidos, y por este motivo resulta muy importante que las compañías prestadoras del servicio puedan filtrar mensajes indeseados antes de enviarlos a su destinatario final. Los mensajes tienen una longitud máxima de 160 caracteres, por lo que el texto resulta poco para realizar la clasificación, en comparación con textos más largos (como los emails). Adicionalmente, los errores de digitación dificultan el proceso de detección automática."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción del problema en términos de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se tiene una muestra contiene 5574 mensajes en inglés, no codificados y clasificados como legítimos (ham) o spam (http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/). La información está almacenada en el archivo `datos/spam-sms.zip`.El problema en términos de los datos consiste en clasificar si un mensaje SMS es legítico o spam, a partir del análisis de las palabras que contiente, partiendo del supuesto de que ciertas palabras que son más frecuentes dependiendo del tipo de mensaje. Esto implica que en la fase de preparación de los datos se deben extraer las palabras que contiene cada mensaje para poder realizar el análsis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aproximaciones posibles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, se desea comparar los resultados de un modelo de redes neuronales artificiales y otras técnicas estadísticas para realizar la clasificación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requerimientos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usted debe:\n",
    "\n",
    "* Preprocesar los datos para representarlos usando bag-of-words.\n",
    "\n",
    "\n",
    "* Construir un modelo de regresión logística como punto base para la comparación con otros modelos más complejos.\n",
    "\n",
    "\n",
    "* Construir un modelo de redes neuronales artificiales. Asimismo, debe determinar el número de neuronas en la capa o capas ocultas.\n",
    "\n",
    "\n",
    "* Utiizar una técnica como crossvalidation u otra similar para establecer la robustez del modelo.\n",
    "\n",
    "\n",
    "* Presentar métricas de desempeño para establecer las bondades y falencias de cada clasificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jfmdd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\jfmdd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jfmdd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import glob\n",
    "import pandas as pd\n",
    "import email\n",
    "import re\n",
    "import sklearn\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_html(message):\n",
    "    return re.sub(\"<[^>]*>\", \"\", message)\n",
    "\n",
    "def re_websites(message):\n",
    "    regex = r'^(http:\\/\\/www\\.|https:\\/\\/www\\.|http:\\/\\/|https:\\/\\/)?[a-z0-9]+([\\-\\.]{1}[a-z0-9]+)*\\.[a-z]{2,5}(:[0-9]{1,5})?(\\/.*)?$'\n",
    "    return re.sub(regex, \"\", message)\n",
    "\n",
    "def re_emails(message):\n",
    "    return re.sub(\"[a-zA-Z0-9-_.]+@[a-zA-Z0-9-_.].+\", \"\", message)\n",
    "\n",
    "def re_specialCharacters(message):\n",
    "    return re.sub(\"[^A-Za-z0-9]+\", \" \", message)\n",
    "\n",
    "def re_numbers(message):\n",
    "    return re.sub(\"[0-9]+\", \"\", message)\n",
    "\n",
    "def re_nonSenseWords(message):\n",
    "    from nltk.corpus import words\n",
    "    words = set(words.words())\n",
    "    \n",
    "    return \" \".join(w for w in nltk.wordpunct_tokenize(message) if w in words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(message):\n",
    "    message = re_html(message)\n",
    "    message = re_websites(message)\n",
    "    message = re_emails(message)\n",
    "    message = re_numbers(message)\n",
    "    message = re_specialCharacters(message)\n",
    "    message = re_nonSenseWords(message)\n",
    "    \n",
    "    return message\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_message(message, lower_case=True, stem=True, stop_words = True, gram=1):\n",
    "    if lower_case:\n",
    "        message = message.lower()\n",
    "    words = word_tokenize(message)\n",
    "    words = [w for w in words if len(w)>2]\n",
    "    if gram >1:\n",
    "        w=[]\n",
    "        for i in range(len(words)-gram+1):\n",
    "            w+=[' '.join(words[i:i+gram])]\n",
    "        return w\n",
    "    if stop_words:\n",
    "        sw=stopwords.words('english')\n",
    "        words=[word for word in words if word not in sw]\n",
    "    if stem:\n",
    "        stemmer=PorterStemmer()\n",
    "        words=[stemmer.stem(word) for word in words]\n",
    "        \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_files(files):\n",
    "    bag_words = []\n",
    "    for filename in files:\n",
    "        with open(filename,'rt',encoding = 'ISO-8859-1') as f:\n",
    "            message = process_file(f.read())\n",
    "            bag_words.append(process_message(message))\n",
    "    return bag_words\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_ham_files1 = glob.glob(\"datos/spam-sms/spam-filter/easy_ham/0*\")\n",
    "easy_ham_files2 = glob.glob(\"datos/spam-sms/spam-filter/easy_ham_2/0*\")\n",
    "hard_ham_files1 = glob.glob(\"datos/spam-sms/spam-filter/hard_ham/0*\")\n",
    "hard_ham_files2 = glob.glob(\"datos/spam-sms/spam-filter/hard_ham_2/0*\")\n",
    "spam_files1 = glob.glob(\"datos/spam-sms/spam-filter/spam/0*\")\n",
    "spam_files2 = glob.glob(\"datos/spam-sms/spam-filter/spam_2/0*\")\n",
    "\n",
    "\n",
    "ehm1 =open_files(easy_ham_files1)\n",
    "ehm2=open_files(easy_ham_files2)\n",
    "hhm1=open_files(hard_ham_files1)\n",
    "hhm2=open_files(hard_ham_files2)\n",
    "sm1=open_files(spam_files1)\n",
    "sm2=open_files(spam_files2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allHWords = []\n",
    "allHWords += ehm1\n",
    "allHWords += ehm2\n",
    "allHWords += hhm1\n",
    "allHWords += hhm2\n",
    "\n",
    "allSWords = []\n",
    "allSWords += sm1\n",
    "allSWords += sm2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Copia de Untitled3.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
